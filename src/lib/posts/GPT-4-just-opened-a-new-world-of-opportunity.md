---
layout: "../../../layouts/BlogPost.astro" #Constant
title: "GPT-4 just opened a new world of possibility"
cardDescription: "GPT-4 was just announced and there are some major improvements..."
description: "OpenAI has recently introduced GPT-4, the latest language model with enhanced capabilities. GPT-4 is reliable, creative, and able to handle much more complex tasks than its predecessor GPT-3.5. It can now accept images as inputs and describe and understand them with incredible accuracy. With the system prompt, developers can choose the character they want Chat GPT to be, making it easier to develop specific chat bot use cases. Additionally, GPT-4 can accept up to 25,000 words, making it easier for programmers and students to prompt the chat bot with documentation and get up-to-date information. The most exciting addition is GPT-4's ability to accept images, enabling endless possibilities for bridging the gap between powerful computing and vision. The future of smart glasses is also mentioned, where individuals can see the history of objects or even hack in real-life sports. Overall, GPT-4 has a wide range of practical upgrades that have the potential to shape the future."
pubDate: "March 15, 2023"
---
Open AI recently [did a livestream](https://www.youtube.com/watch?v=outcGtbnMuQ) introducing GPT-4 and the new capabilities that come with it. Along with advancing current chat capabilities (especially in doing math and oddly, taxes), the new GTP-4 model has come with a plethora of practical upgrades and can now accept images as inputs and from what was shown in the video, describe and understand them with incredible accuracy. 

### What’s new

In normal conversation and common questions, GPT-3.5 and GPT-4 perform similarly, but

> “The difference comes out when the complexity of the task reaches a sufficient threshold—GPT-4 is more reliable, creative, and able to handle much more nuanced instructions than GPT-3.5.” - [openai.com](https://openai.com/research/gpt-4)
    
For example, in the stream, GPT-3.5 was unable to summarize some text using words that all start with the same letter, while GPT-4 did this flawlessly. More notably, They compared the ability of GPT-3.5 and GPT-4 through a series of tests, including all of the offered AP Tests. While performing miles better than GPT-3.5 on math exams like AP Calculus BC, GPT-4 oddly continued to do poorly on the English language and literature exams earning 2’s on both AP exams, same as GPT-3.5. I’ll be out of school soon enough so all of that isn’t super exciting to me, but here’s what is.


### System prompt

with the new GPT-4 model, you can now tell Chat GPT what it is and it will respond in character to who you want it to be. In the demo, they prompted the chat bot to be “TaxGPT” the tax expert. It then responded to all of the prompts in character and the host of the preview made it apparent that these prompts helped a lot in getting correct and relevant information. On the same note, it’s now easier to develop specific chat bot use cases, as the system prompt can be whatever the developer choses and remain constant. Before, chatting with [Rick Sanchez](https://beta.character.ai/chat?char=I5CtZPUuEYrGThGxRuVSN3l01sn6DjND2T-cTsk-jrA) and things of that nature was much more difficult because each message had to include a prompt saying who they wanted the bot be. Now, with system prompts, this has been made much simpler and is a parameter that can be used in the API.

### Longer prompts

GPT-4 and GPT-3.5 accept 25,000 and 3,000 words respectively. While a giant 8x increase may not seem like big news to someone using the model for generating emails, essays, and other smaller prompts, there are huge benefits for programmers and students, both of which I am. GPT-4 is still trained on the same material, all of which is from 2021 and before. Because of this, when I’m working with new tech like Svelte, SvelteKit, and Astro (My recent choices), Chat GPT doesn’t offer a ton of help. To remedy this, you can prompt the chat bot with documentation from the respective website and then ask questions for up to date information. That power has now been drastically increased, allowing for a more full set of instructions, and more context for the bot to work with. As for being a student, finding a chapter of a book and then getting a summary is now easier than ever. You don’t even need to find the specific information, just drop the whole chapter in and have GPT-4 parse through the information and make a study guide for you. On a side note, Chat GPT has been exceptionally poor at doing linear algebra so hopefully that changes here.

### Accepting Images

This is by far the most exciting addition for me. Not because of what it can currently do, but what it can be used for. The future that is made possible by processing images through such a powerful system is beyond all of our imagination. Even just passing random images, it was able to explain in great depth the details of the images, and could answer questions regarding the images. In addition, I have been using ChatGPT for front end development and, while it still does a good job, it is more suited for backend or procedural programming. With this new update, though, GPT-4 will be able to share my vision and help bring me exactly what I envision. I cannot wait until the API is public to see what the community comes up with given this new processing power. I don’t have a ton of thoughts regarding what was shown in the preview, but the sheer possibility opened between bridging the gap between powerful computing and vision is something that has the ability to shape the future.

#### off topic

Smart glasses are coming soon and I’m beyond stoked. Imagine looking at a building and seeing the history, looking at the road and seeing the directions, or even looking at someone and understanding their emotion from their body language. All of this can be possible and harnessed into some lenses and I cannot wait until that day comes. Imagine hacking in real life sports. This is hardly even related at this point, but imagine you’re playing baseball or football and can see the trajectory of the ball in real time. Nuts to think about. Anyway, I digress.
